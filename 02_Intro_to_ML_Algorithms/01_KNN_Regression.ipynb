{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ijNtWMma6lh"
      },
      "source": [
        "# k-Nearest Neighbors Regression\n",
        "## DS-3001: Machine Learning 1\n",
        "\n",
        "Content adapted from Terence Johnson (UVA)\n",
        "\n",
        "**Notebook Summary**: In this notebook, we discuss KNN regression. This is a simple machine learning technique for predicting a continuous numerical outcome variable. We discuss the similarities and differences between KNN regression and classification. As well, we discuss how we can quantify our errors in a regression task and how we can use that quantification for hyperparameter selection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0prE5Cua-yi"
      },
      "source": [
        "#### Setting Up Our Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s0WKgRyQa2iv"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os # For changing directory\n",
        "\n",
        "# To mount your google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AqnRtPkDbCkw"
      },
      "outputs": [],
      "source": [
        "# path_to_DS_3001_folder = '/content/drive/MyDrive/DS-3001/02_Intro_to_ML_Algorithms'\n",
        "path_to_DS_3001_folder = ''\n",
        "\n",
        "# Update the path to your folder for the class\n",
        "# Where you stored the data from the previous noteboook\n",
        "# path_to_DS_3001_folder = ''\n",
        "os.chdir(path_to_DS_3001_folder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRSw-b5bbPQY"
      },
      "source": [
        "### Recap From Last Time and Changes for Today\n",
        "\n",
        "- In the KNN Classification notebook, we were introduced to a supervised learning task where we used some features/covariates $X$ to predict some outcome $y$.\n",
        "- The task we looked at was predicting diabetes outcome, which was a categorical feature, making it a **classification** task.\n",
        "- We looked at our model's performance using the confusion matrix and accuracy measures.\n",
        "- Today, we'll look at slightly different task: **regression.** For regression, we are interested in predicting a numeric outcome variable. In other words, $y$ will be a continuous variable instead of a categorical class label.\n",
        "- Today, we will look at tools to model data in a regression setting and how to measure our performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_CEvPT5cGiO"
      },
      "source": [
        "# 1. Preparing the Data\n",
        "### Introduction to the Data for Today\n",
        "\n",
        "- Today we're looking at a data set about cars. Each observation is a unique make and model of a car. We have a variety of features to choose from. We're going to attempt to predict the `baseline sales` ($y$) of the car given the car's `baseline price` and `baseline mpg` (x). A more complete description of our variables of interest are given below:\n",
        "  - `baseline price`: The market price of the car.\n",
        "  - `baseline mpg`: The manufacturer's claim of miles per gallon.\n",
        "  - `baseline sales`: The predicted sales of the car."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NVdoWlVncCYy"
      },
      "outputs": [],
      "source": [
        "# Load in our data set as a Pandas Data Frame\n",
        "cars_df = pd.read_csv('data/cars_env.csv')\n",
        "cars_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1FLMNFN2_1L"
      },
      "source": [
        "#### Quick Data Cleaning for our `baseline price` and `baseline mpg` variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ykwZHDnndcPA"
      },
      "outputs": [],
      "source": [
        "# Look at the distribution of the price\n",
        "# Should we make any changes?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YLyRcHIjdtMS"
      },
      "outputs": [],
      "source": [
        "# Look at the distribution of MPG\n",
        "# should we make any changes?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7LCwtAR3_kW"
      },
      "source": [
        "### How do our chosen variables vary with the `baseline sales`?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2lFf5ffH3jvT"
      },
      "outputs": [],
      "source": [
        "# How can we look at a comparison between the two numeric values visually?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVYQSJwW48d6"
      },
      "source": [
        "### How do our chosen variables vary with one another, and is there a distinct seperation of the sales value?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DODGHSoH7cPo"
      },
      "outputs": [],
      "source": [
        "import matplotlib as mpl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-qr48bRB46ci"
      },
      "outputs": [],
      "source": [
        "# Create a scatterplot between the baseline mpg and price, colored by the baseline sales\n",
        "ax = sns.scatterplot(\n",
        "    x = cars_df['baseline mpg'],\n",
        "    y = cars_df['baseline price'],\n",
        "    hue = cars_df['baseline sales'].values,\n",
        "    palette = 'crest',\n",
        "    alpha = 0.5,\n",
        "    legend = False\n",
        ")\n",
        "plt.xlabel('Baseline MPG')\n",
        "plt.ylabel('Baseline Price')\n",
        "\n",
        "# Creating a colorbar\n",
        "norm = mpl.colors.Normalize(\n",
        "    vmin=cars_df[\"baseline sales\"].min(),\n",
        "    vmax=cars_df[\"baseline sales\"].max()\n",
        ")\n",
        "\n",
        "# Create scalar mappable\n",
        "sm = mpl.cm.ScalarMappable(norm=norm, cmap=\"crest\")\n",
        "sm.set_array([])\n",
        "\n",
        "# Add colorbar\n",
        "ax.figure.colorbar(sm, ax=ax, label=\"Baseline Sales\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dAn-_x48b2k"
      },
      "source": [
        "# 2. Regression\n",
        "\n",
        "## k-NN Regression\n",
        "\n",
        "* In KNN regresssion, we use a similar set up as we had for KNN Classification but with a couple of tweaks.\n",
        "* We are still working with covaraites/features $X = [x_1, x_2, ..., x_L]$, consisting of $N$ observations of $L$ variables, and an observed outcomes/target variable $Y$. Because we have switched to regresssion, our outcome $Y$ is **numeric,** not categorical.\n",
        "* We are intereseted in when we get a new case $\\hat{x} = (\\hat{x}_1,...,\\hat{x}_L)$. We want to predict what **numeric value** the outcome will likely take, $\\hat{y}$.\n",
        "* With this set up, the $k$ Nearest Neighbor Regression Algorithm works as follows:\n",
        "  1. Compute the distance from $\\hat{x}$ to each observation $x_i$ in the data set.\n",
        "  2. Find the $k$ \"nearest neighbors\" $x_1^*$, $x_2^*$, ..., $x_k^*$ to $\\hat{x}$ in the data in terms of distance, with values $y_1^*$, $y_2^*$, ..., $y_k^*$.\n",
        "  3. Return the *average* of the neighbor values,\n",
        "  \\begin{gather}\n",
        "    \\hat{y}(\\hat{x}) = \\dfrac{y_1^* + y_2^* + ... + y_k^*}{k} = \\frac{1}{k} \\sum_{i=1}^k y_i^*\n",
        "  \\end{gather}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tu1pIy1E9yYq"
      },
      "source": [
        "### Implementing k-NN Regression in Scikit-Learn\n",
        "\n",
        "* To implement KNN regression in Scikit-Learn, you can import the model similarly to how we did for the KNN Classifier\n",
        "  - `from sklearn.neighbors import KNeighborsRegressor`\n",
        "* We will use the same workflow we discussed for the KNN Classifier when using Scikit-Learn:\n",
        "  1. Create an untrained model object with a fixed $k$:\n",
        "    - `model = KNeighborsRegressor(n_neighbors=k)`\n",
        "  2. Fit that object to the data, $(X,y)$:\n",
        "    - `fitted_model = model.fit(X, y)`\n",
        "  3. Use the fitted object to make predictions for new cases $\\hat{x}$\n",
        "    - `y_hat = fitted_model.predict(x_hat)`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0In1biV-uA2"
      },
      "source": [
        "### Seperating our data into a Train/Test Split\n",
        "\n",
        "* We discussed in the KNN classification notebook the need to seperate our data into a Train/Test split. We train our model using the training data and test the model (selecting hyperparameters, evaluating performance, etc.) using the test data set.\n",
        "* This allows us to simulate how our model may perform on unseen data that were created using the same data generation process.\n",
        "* We care more about how our model does on unseen data than we do about how it performs on the data it was trained on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESP-ytvB_Kgi"
      },
      "source": [
        "### Implementing KNN Using our Cars Data Set\n",
        "\n",
        "#### 1. Imports and Function Definitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9aXf4aKL7Wxd"
      },
      "outputs": [],
      "source": [
        "# Our necessary imports for SKLearn\n",
        "from sklearn.neighbors import KNeighborsRegressor # The model object\n",
        "from sklearn.model_selection import train_test_split # Creating a train test split\n",
        "\n",
        "# Our MinMaxScaler function from last time\n",
        "# scaling the variables is critical since we are working with distances\n",
        "def MinMaxScaler(x):\n",
        "\n",
        "  # Pre-compute the min and max of the variable\n",
        "  min_x = np.min(x)\n",
        "  max_x = np.max(x)\n",
        "\n",
        "  # Calculate the newly scaled version of the variable\n",
        "  u = (x - min_x) / (max_x - min_x)\n",
        "\n",
        "  # Return the scaled version of the value\n",
        "  return u"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPo7maPJ_upW"
      },
      "source": [
        "#### 2. Normalizing our Variables and Creating a Train/Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "312Nc-1P_m0u"
      },
      "outputs": [],
      "source": [
        "# Select our outcome variable, baseline sales in this case\n",
        "y = cars_df['baseline sales']\n",
        "\n",
        "# Select our variables of interest\n",
        "var1 = 'baseline mpg'\n",
        "var2 = 'baseline price'\n",
        "x = cars_df.loc[:, [var1, var2]] # Creating our x pandas series\n",
        "\n",
        "# Scale our variables so that they are on the same scale, a crucial step\n",
        "u = x.apply(MinMaxScaler)\n",
        "\n",
        "# Create a train test split\n",
        "u_train, u_test, y_train, y_test = train_test_split(u, y, test_size = 0.2, random_state = 123)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_G8O8fJhQBhg"
      },
      "outputs": [],
      "source": [
        "# Look at our scaled distributions\n",
        "sns.histplot(\n",
        "  u['baseline mpg'],\n",
        "  label = 'Baseline MPG',\n",
        "  color = 'dodgerblue',\n",
        "  bins = 50,\n",
        "  alpha = 0.5\n",
        ")\n",
        "sns.histplot(\n",
        "  u['baseline price'],\n",
        "  label = 'Baseline Price',\n",
        "  color = 'firebrick',\n",
        "  bins = 50,\n",
        "  alpha = 0.5\n",
        ")\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8BhldPBAiIx"
      },
      "source": [
        "#### 4. Fit the KNN Regeression Model and make Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r4AqUUy_AZl8"
      },
      "outputs": [],
      "source": [
        "# Pick a value of k to start\n",
        "k = 5\n",
        "\n",
        "# Create a model instance\n",
        "model = KNeighborsRegressor(n_neighbors = k)\n",
        "\n",
        "# Fit the model to our TRAIN data\n",
        "model = model.fit(u_train, y_train)\n",
        "\n",
        "# Make predictions on the test data set\n",
        "y_hat = model.predict(u_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2x8iiZNBE2J"
      },
      "source": [
        "#### 5. Visualize our Predicted Outcome vs the True Outcome"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BuYe3vhYBDJS"
      },
      "outputs": [],
      "source": [
        "# Creating a Scatter Plot\n",
        "ax = sns.scatterplot(\n",
        "    x = y_test,\n",
        "    y = y_hat\n",
        ")\n",
        "ax.set_aspect('equal')\n",
        "\n",
        "# Setting what the y and x limits should be\n",
        "ax_min = 0\n",
        "ax_max = max(y_test.max(), y_hat.max())\n",
        "plt.ylim([ax_min, ax_max])\n",
        "plt.xlabel('True Test Outcome')\n",
        "plt.ylabel('Predicted Test Outcome')\n",
        "plt.title('Comparison of Predicted vs. True Outcome on Test Data Set')\n",
        "plt.xticks(rotation = 45) # Change the rotation of the tick labels so that they don't overlap\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pV8DZJHzBjLZ"
      },
      "source": [
        "**Question:** What shape would we like to see in the plot before? In other words, if our model was perfect at predicting the outcome, what would the graph look like?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VGItTLpPBZWu"
      },
      "outputs": [],
      "source": [
        "# Add that shape as a reference to the plot above\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWo4AVo9C4zd"
      },
      "source": [
        "# 3. Residuals and MSE\n",
        "\n",
        "## Residuals\n",
        "\n",
        "* The **residual** is the distance between the true value ($y$) and the predicted value ($\\hat{y}$).\n",
        "\n",
        "\\begin{gather}\n",
        "  \\underbrace{r_i}_{\\text{Residual, error}} = \\underbrace{y_i}_{\\text{True}} - \\underbrace{\\hat{y}(x_i)}_{\\text{predicted}}\n",
        "\\end{gather}\n",
        "\n",
        "* This tells us how far our predicted value is from the true value. This can be interpreted as the error of our prediction.\n",
        "\n",
        "* This is the performance metric for regression in the same way that the confuson matrix was for classification.\n",
        "\n",
        "* It is helpful to look at the residuals to understand how our model did"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8T7ObjPVCwtr"
      },
      "outputs": [],
      "source": [
        "# Let's visualize our residuals\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Turn our pandas series into Numpy arrays\n",
        "y_test = np.asarray(y_test)\n",
        "y_hat = np.asarray(y_hat)\n",
        "\n",
        "# Add in our data points\n",
        "plt.scatter(\n",
        "    y_test,\n",
        "    y_hat,\n",
        "    label = 'Our Data'\n",
        ")\n",
        "\n",
        "# We can get the perfect prediction line which\n",
        "ax_min = min(y_test.min(), y_hat.min())\n",
        "ax_max = max(y_test.max(), y_hat.max())\n",
        "line = np.linspace(0, ax_max, 1000)\n",
        "\n",
        "# Visualizing the Perfect Prediction Line\n",
        "plt.plot(\n",
        "    line,\n",
        "    line,\n",
        "    color = 'black',\n",
        "    linestyle = '--',\n",
        "    alpha = 0.2,\n",
        "    label = r'y = $\\hat{y}$'\n",
        ")\n",
        "\n",
        "# Plot the Residuals: vertical lines y_i - y_hat_i\n",
        "for a, b in zip(y_test, y_hat):\n",
        "    plt.vlines(\n",
        "        a, a, b,\n",
        "        color='red', linewidth=.5,\n",
        "        alpha=0.5, linestyle = '--'\n",
        "    )\n",
        "\n",
        "# Add one more empty line to just include in the legend\n",
        "plt.vlines(\n",
        "    a, a, b,\n",
        "    color='red', linewidth=.5,\n",
        "    alpha=0.5, linestyle = '--',\n",
        "    label = r'$r = y - \\hat{y}$'\n",
        ")\n",
        "\n",
        "plt.xlabel('True Y Outcome')\n",
        "plt.ylabel('Predicted Y Outcome')\n",
        "plt.title('Plotting Residuals')\n",
        "plt.xticks(rotation = 45)\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ycOti3VGall"
      },
      "source": [
        "#### Plotting the Residuals on the y-axis instead of our predicted outcome"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oBII0OOYE6C6"
      },
      "outputs": [],
      "source": [
        "# Let's visualize our residuals\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Turn our pandas series into Numpy arrays\n",
        "y_test = np.asarray(y_test)\n",
        "y_hat = np.asarray(y_hat)\n",
        "\n",
        "# Calculate our residual\n",
        "residual = y_test - y_hat\n",
        "\n",
        "# Add in our data points\n",
        "plt.scatter(\n",
        "    y_test,\n",
        "    residual\n",
        ")\n",
        "\n",
        "# Add a horizontal line at 0\n",
        "plt.axhline(\n",
        "    y = 0,\n",
        "    linestyle = '--',\n",
        "    color = 'black',\n",
        "    alpha = 0.2,\n",
        "    label = 'Prefect Prediction'\n",
        ")\n",
        "\n",
        "# Plot the Residuals: vertical lines y_i - y_hat_i\n",
        "for a, b in zip(y_test, residual):\n",
        "    plt.vlines(\n",
        "        a, b, 0,\n",
        "        color='red', linewidth=.5,\n",
        "        alpha=0.5, linestyle = '--'\n",
        "    )\n",
        "\n",
        "# Add one more empty line to just include in the legend\n",
        "plt.vlines(\n",
        "    a, a, b,\n",
        "    color='red', linewidth=.5,\n",
        "    alpha=0.5, linestyle = '--',\n",
        "    label = r'$r = y - \\hat{y}$'\n",
        ")\n",
        "\n",
        "plt.xlabel('True Y Outcome')\n",
        "plt.ylabel(r'Residual ($r = y - \\hat{y}$)')\n",
        "plt.title('Plotting Residuals')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQ3rvdHNH4zN"
      },
      "source": [
        "## Loss Function: Mean Squared Error\n",
        "\n",
        "* Similar to how accuracy was a one-number summary of the confusion matrix for classification, **mean squared error** is a one number summary of how we did at regression.\n",
        "* We compute the **mean squared error (MSE)** as\n",
        "\n",
        "\\begin{gather}\n",
        "  \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2\n",
        "\\end{gather}\n",
        "\n",
        "* An alternative measure is the **root mean squared error (RMSE)** which is computed as:\n",
        "\n",
        "\\begin{gather}\n",
        "  \\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2}\n",
        "\\end{gather}\n",
        "\n",
        "* This gives us the distance from the true values to the predicted one, weighted by the sample size. As the number of observations (n) gets large, these values typically approach some fixed value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdCxGe8GIoiG"
      },
      "source": [
        "### Creating a Mean Squared Error Function\n",
        "\n",
        "* In the classification notebook we used `model.score(y_hat, y_test)` to understand how our model performed. `model.score(y_hat, y_test)` does not return either the MSE or RMSE for a regression task. Instead, it returns the $R^2$ value, which we'll cover in the future.\n",
        "\n",
        "* Because of this, we need to write our own function for the MSE or we could use use the alternative skelarn function:\n",
        "  - `from sklearn.metrics import mean_squared_error`\n",
        "\n",
        "* For today, we're going to write our own function to see how we can implement the math formula in Python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6NiE_iVuFM0j"
      },
      "outputs": [],
      "source": [
        "# First, let's create our own funciton\n",
        "\n",
        "def mse(y_test, y_hat):\n",
        "\n",
        "  # Calculate the squared errors\n",
        "  squared_errors = (y_hat - y_test)**2\n",
        "\n",
        "  # Calculate the mean of the squared errors\n",
        "  mean_squared_errors = np.sum(squared_errors) / len(y_test)\n",
        "\n",
        "  return mean_squared_errors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JA3pDyV0Jsm-"
      },
      "outputs": [],
      "source": [
        "# Apply the function to our current case\n",
        "test_mse = mse(y_hat, y_test)\n",
        "test_rmse = np.sqrt(test_mse)\n",
        "\n",
        "print('Test MSE:', test_mse)\n",
        "print('Test RMSE:', test_rmse)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1H7KR_2EKWln"
      },
      "source": [
        "### Picking k (our hyperparameter)\n",
        "\n",
        "* We can make a comparison between the performance of the model on the train and test data for different values of k. This will let us know how our model performs for different values of k.\n",
        "* By plotting both the train and test MSE, we can see if the model is overfitting or underfitting on our training data.\n",
        "* As well, we can identify the model that minimizes the MSE on the test data set and use that value of k for our model going forward."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "inRP0U7nJ-Hm"
      },
      "outputs": [],
      "source": [
        "k_grid = [ (2 * k + 1) for k in range(100) ] # Look at odd k's\n",
        "\n",
        "mse_train_by_k = []\n",
        "mse_test_by_k = []\n",
        "\n",
        "# Loop over the values of k\n",
        "for k in k_grid:\n",
        "\n",
        "  # Create a model instance\n",
        "  model = KNeighborsRegressor(n_neighbors = k)\n",
        "\n",
        "  # Fit the model to our TRAIN data\n",
        "  model = model.fit(u_train, y_train)\n",
        "\n",
        "  # Make predictions on the train and test data set for comparison\n",
        "  y_hat_train = model.predict(u_train)\n",
        "  y_hat_test = model.predict(u_test)\n",
        "\n",
        "  # Calculate the mse for the train and test\n",
        "  mse_train = mse(y_hat_train, y_train)\n",
        "  mse_test = mse(y_hat_test, y_test)\n",
        "\n",
        "  # Append the train and test mse to the lists\n",
        "  mse_train_by_k.append(mse_train)\n",
        "  mse_test_by_k.append(mse_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UouStnoLMDXD"
      },
      "outputs": [],
      "source": [
        "# Visualize the results\n",
        "\n",
        "# Plotting the train MSE\n",
        "sns.lineplot(\n",
        "    x = k_grid,\n",
        "    y = mse_train_by_k,\n",
        "    label = 'Training MSE'\n",
        ")\n",
        "\n",
        "# Plotting the test MSE\n",
        "sns.lineplot(\n",
        "    x = k_grid,\n",
        "    y = mse_test_by_k,\n",
        "    label = 'Testing MSE'\n",
        ")\n",
        "\n",
        "plt.xlabel('k')\n",
        "plt.ylabel('MSE')\n",
        "plt.title('MSE vs. k')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xt8Cl-oqMlm4"
      },
      "source": [
        "**Question:** What value of k minimizes the MSE for the test data set?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J4DPvzHoMV99"
      },
      "outputs": [],
      "source": [
        "# Find the value for k that minimizes the MSE on the test data set\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmH-vJ8-NEaZ"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "* Through the past notebooks, we have completed a full data science loop:\n",
        "  1. Wrangle the data\n",
        "  2. EDA and Visualize to view relationships\n",
        "  3. KNN Regression for numeric response data, and KNN classification to predict categorical response data\n",
        "  4. Train/Test split for hyperparameter selection\n",
        "\n",
        "* We will continue to iterate on this process throughout the course."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
