{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# k Means Clustering\n",
        "## DS-3001: Machine Learning 1\n",
        "\n",
        "Content adapted from Terence Johnson (UVA)\n",
        "\n",
        "**Notebook Summary**: In this notebook, we introduce the concept of clustering as an unsupervised learning task. In clustering, we are interested in discovering some underlying groups (clusters) in our data that we do not have information about beforehand. In other words, we don't know how many groups we have or necessarily what the groups represent. In this notebook we specifically look at KMeans clustering and how we can use it to find underlying structure in Virginia Electrcity Sales data. We discuss how the KMeans algorithm operates, how changing the value of k changes our results, and how we can identify the best value of k for our data."
      ],
      "metadata": {
        "id": "EDM-jSEeVMye"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Setting Up Our Environment"
      ],
      "metadata": {
        "id": "IDzCyehWVezF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35N98vGvVGKG"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os # For changing directory\n",
        "\n",
        "# To mount your google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_DS_3001_folder = '/content/drive/MyDrive/DS-3001/02_Intro_to_ML_Algorithms'\n",
        "# path_to_DS_3001_folder = ''\n",
        "\n",
        "# Update the path to your folder for the class\n",
        "# Where you stored the data from the previous noteboook\n",
        "os.chdir(path_to_DS_3001_folder)"
      ],
      "metadata": {
        "id": "uvhe_37OVg81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clustering\n",
        "\n",
        "- Last week, we discussed the k Nearest Neighbor method for *classification* (predicting categorial values for new data) and *regression* (predicting numeric values for new data).\n",
        "- The cases of classification and regression are examples of **supervised learning**. We know what success looks like because we have the true answers for the training data.\n",
        "- Today, we're looking at an **unsupervised learning** algorithm where we don't necessarily know what success looks like. Instead, we're looking for general patterns in the data without defining what success looks like in advance.\n",
        "- The main algorithm we're looking at for today is *k-means clustering*."
      ],
      "metadata": {
        "id": "1RWzoeWsV7Lt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unsupervised Learning\n",
        "\n",
        "- In **unsupervised learning**, we do not have an outcome variable $y$ for each observation. There is not a single outcome that we are trying to predict, but rather we are looking for meaningful patterns within the data.\n",
        "- The idea is that there is a *latent structure* with a discrete characteristic in the data we are trying to recover."
      ],
      "metadata": {
        "id": "KnoGvijVWwo0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Examples of Data Where Unsupervised Learning is Applicable\n",
        "\n",
        "- When we want to detect anomalies in some financial patterns.\n",
        "  - Ex. Identifying collusive bidders vs competittive bidders in an auction.\n",
        "  - Ex. Identifying fraud in banking activity.\n",
        "\n",
        "- Identifying different communities from interaction data.\n",
        "\n",
        "- Identifying different strains of disease based on patient outcomes."
      ],
      "metadata": {
        "id": "Ey3upoBPXsK-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data for Today: VA Electrcity Sales\n",
        "\n",
        "- Today we are going to look at data that describes the electricity consumption in Virginia.\n",
        "- We are interested in finding some underlying structure in the data given two variables:\n",
        "  - **price:** The price of electricity. The units are cents per kilowatthour.\n",
        "  - **sales:** The amount of electricity sold. The units are milllion kilowatthours."
      ],
      "metadata": {
        "id": "IxVu_jQCX0HE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "el_df = pd.read_csv('./data/electricity_data_validation.csv')\n",
        "el_df.head()"
      ],
      "metadata": {
        "id": "8zpBHPi7Wv4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean the price and sales variables\n"
      ],
      "metadata": {
        "id": "HIC_FrhldD4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Look at a comparison between price and sales\n",
        "# Does there seem to be an underlying relationship?\n",
        "\n",
        "sns.scatterplot(\n",
        "  x = el_df['price'],\n",
        "  y = el_df['sales']\n",
        ")\n",
        "plt.title('Commparison Between Price and Sales')\n",
        "plt.xlabel('Price (cents / kWh)')\n",
        "plt.ylabel('Sales (million kWh)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Qn-8j-Z6ZCfh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question:** Looking at the plot above, are there clear clusters in the data? How many clusters do you think there are?"
      ],
      "metadata": {
        "id": "OQ2vdFIt4c2l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unsupervised Learning Continued\n",
        "\n",
        "- We are interested in the following question: *can we identify some label / group for each observation without initially knowing which group each point / observation belongs to?*\n",
        "- For this problem, we use the same idea we used for KNN.\n",
        "  - If two points were created by the same data generating process/cluster, then their values are probably close together.\n",
        "  - If there are a discrete number of distinct data generating processes, we should be able to recover their values by looking at separation between the groups."
      ],
      "metadata": {
        "id": "IRA6Lshffzeb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Quick numeric example with data geneation processes\n",
        "\n",
        "# Define different means and standard deviations\n",
        "# for the different generation processes\n",
        "mean1 = 0\n",
        "mean2 = 10\n",
        "std1 = 1\n",
        "std2 = 3\n",
        "\n",
        "n_per_k = 100\n",
        "\n",
        "# Generate data using 2 different data generation processes\n",
        "np.random.seed(2302)\n",
        "x1 = np.random.normal(mean1, std1, n_per_k)\n",
        "y1 = np.random.normal(mean1, std1, n_per_k)\n",
        "x2 = np.random.normal(mean2, std2, n_per_k)\n",
        "y2 = np.random.normal(mean2, std2, n_per_k)\n",
        "\n",
        "# Combine data into a single data set\n",
        "x = np.vstack([x1, x2]).flatten()\n",
        "y = np.vstack([y1, y2]).flatten()\n",
        "\n",
        "# Plot the data now without labels\n",
        "plt.scatter(x, y)\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.title('Comparison of Data with\\nDifferent Data Generation Processes')\n",
        "plt.show()\n",
        "\n",
        "# Plot the example with true labels\n",
        "plt.scatter(x[:n_per_k], y[:n_per_k], color = 'dodgerblue', label = 'Data Generation Process 1')\n",
        "plt.scatter(x[n_per_k:], y[n_per_k:], color = 'firebrick', label = 'Data Generation Process 2')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.title('Comparison of Data with\\nDifferent Data Generation Processes')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BS952SfCgxP4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## k-Means Clustering Algorithm (k-MC)\n",
        "\n",
        "- The k-Means Clustering algorithm is defined as follows:\n",
        "\n",
        "  0. (Initialization) Randomly select $k$ points to be the *centroids*, $\\{c_1, c_2, ..., c_k\\}$\n",
        "  1. Find the distance of each observation $x_i$ to each centroid $c_j$\n",
        "  2. Assign each point $i$ to the closest centroid $j$\n",
        "  3. Compute the new value of each centroid $j$ as the average of all of the observations $i$ assigned to it\n",
        "  4. (Convergence) Repeat steps 1-3 until the observations are assigned to the same centroids twice (no data point switches centroid assignment), or a maximum number of iterations is reached.\n",
        "\n",
        "- k-MC is one of many clustering algorithms. What is nice about it is that, despite it being computationally intensive to calculate all these distances, it does scale to very large datasets. Likewise, a lot of the steps in using k-MC appear in other, similar algorithms (e.g. scree plot for spectral clustering)\n",
        "\n",
        "\n",
        "#### Distance and Scaling\n",
        "\n",
        "* Similarily to KNN, KMC uses the Euclidean distance to find the distance between a new case $\\hat{x}$ and each observation $x_i$.\n",
        "\n",
        "\\begin{gather}\n",
        "  d(\\hat{x},x_i) = \\sqrt{ \\sum_{\\ell=1}^N (\\hat{x}_i - x_i)^2}\n",
        "\\end{gather}\n",
        "\n",
        "* Again, we need to scale the variables so that no one variable dominates the distance calculation between two observations.\n",
        "\n",
        "\\begin{gather}\n",
        "   u_i = \\dfrac{x - \\min(x_i)}{\\max(x)-\\min(x)}\n",
        "\\end{gather}\n",
        "\n",
        "#### SciKit Learn Implementation\n",
        "\n",
        "* SciKit learn has a function for k Means that we can use.\n",
        "  - `from sklearn.cluster import KMeans`\n",
        "* You can use the following arguments when creating the model instance:\n",
        "  - `n_clusters = k`: The number of clusters / centroids to use.\n",
        "  - `n_init = 10`: The number of test runs to do. Each test run has a different set of initial centroids. The model will return the \"best\" clustering result.\n",
        "  - `max_iter = 300`: The maximum number of iterations to run for the KMC algoritm.\n",
        "  - `random_state = None`: An initial state for the random generation so that the results are replicable.\n",
        "* You can create the KMC model in the same way we did with KNN:\n",
        "  1. Use `model = KMeans(n_clusters, n_init)` to create the model.\n",
        "  2. Use the `.fit(X)` method to fit the model to the data $X$.\n",
        "  3. Use the `.predict(X_hat)` method to predict cluster values for new classes `X_hat`.\n"
      ],
      "metadata": {
        "id": "lE42MaM_g18g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First, import the kmeans function from sklearn\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Define a minmax scaler as we did for KNN\n",
        "def MinMaxScaler(x):\n",
        "\n",
        "  # Pre-compute the min and max of the variable\n",
        "  min_x = np.min(x)\n",
        "  max_x = np.max(x)\n",
        "\n",
        "  # Calculate the newly scaled version of the variable\n",
        "  u = (x - min_x) / (max_x - min_x)\n",
        "\n",
        "  # Return the scaled version of the value\n",
        "  return u"
      ],
      "metadata": {
        "id": "dvI1szsZg068"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create new columns for our scaled values\n",
        "el_df['price_scaled'] = MinMaxScaler(el_df['price'])\n",
        "el_df['sales_scaled'] = MinMaxScaler(el_df['sales'])\n",
        "\n",
        "# Pull out the minimum and maximum for each variable\n",
        "# need these to transform are values back to the unscaled version\n",
        "min_price = np.min(el_df['price'])\n",
        "max_price = np.max(el_df['price'])\n",
        "\n",
        "min_sales = np.min(el_df['sales'])\n",
        "max_sales = np.max(el_df['sales'])\n",
        "\n",
        "# Create an X value for sklearn\n",
        "X = el_df.loc[:, ['price_scaled', 'sales_scaled']]\n",
        "X.head()"
      ],
      "metadata": {
        "id": "6r9GyRIMkilG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting scaled values\n",
        "sns.scatterplot(\n",
        "    data = el_df,\n",
        "    x = 'price_scaled',\n",
        "    y = 'sales_scaled'\n",
        ")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "v0EGG7K9lJsm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Setting Up k Means Clustering Algorithm\n",
        "\n",
        "##### Iteration 1"
      ],
      "metadata": {
        "id": "v-C_-H4el1jQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Set a seed for reproducibility\n",
        "random_state_value = 12\n",
        "np.random.seed(random_state_value)\n",
        "k = 5 # Fill this in with your guess from above\n",
        "max_iter = 1 # Start with only one iteration\n",
        "\n",
        "# 2. Create some initial centroids that are randomly defined\n",
        "initial_centroids = np.random.rand(k, 2)\n",
        "\n",
        "# 3. Create a model instance for the KMeans\n",
        "model = KMeans(\n",
        "  n_clusters = k, # The number of centroids / groups\n",
        "  max_iter = max_iter, # The number of iterations to run\n",
        "  init = initial_centroids, # Pass in our initial centroids\n",
        "  random_state = random_state_value # The random state for reproducibility\n",
        ")\n",
        "\n",
        "# 4. Fit the model to our data\n",
        "model = model.fit(X)\n",
        "\n",
        "# 5. Gather predictions\n",
        "el_df['g_hat'] = model.predict(X) # Predicted group\n",
        "\n",
        "# 6. Re-normalize the centers so that\n",
        "# they're on the same scale as our original data\n",
        "centers = model.cluster_centers_ # The computed centers\n",
        "centroid_price = centers[:, 0] * (max_price - min_price) + min_price\n",
        "centroid_sales = centers[:, 1] * (max_sales - min_sales) + min_sales\n",
        "\n",
        "# 7. Plot the data and their centers\n",
        "sns.scatterplot(\n",
        "    data = el_df,\n",
        "    x = 'price',\n",
        "    y = 'sales',\n",
        "    hue = 'g_hat', # Color points based on predicted group\n",
        "    style = 'g_hat'\n",
        ")\n",
        "plt.title('Centroids and Group Predictions After 1 Iterations')\n",
        "\n",
        "## Ploting the centroids\n",
        "plt.scatter(\n",
        "    centroid_price,\n",
        "    centroid_sales,\n",
        "    color = 'red',\n",
        "    label = 'centroids'\n",
        ")\n",
        "\n",
        "# Add a legend and show\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Fzm3ivd9lzBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What if we use a different initializaiton?\n",
        "\n",
        "**Change the `random_state_value` to have different intializations**. Notice how the centroid placement changes."
      ],
      "metadata": {
        "id": "D3e4S_jX0g2G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Set a seed for reproducibility\n",
        "random_state_value = 10923\n",
        "np.random.seed(random_state_value)\n",
        "k = 5 # Fill this in with your guess from above\n",
        "max_iter = 1 # Start with only one iteration\n",
        "\n",
        "# 2. Create some initial centroids that are randomly defined\n",
        "initial_centroids = np.random.rand(k, 2)\n",
        "\n",
        "# 3. Create a model instance for the KMeans\n",
        "model = KMeans(\n",
        "  n_clusters = k, # The number of centroids / groups\n",
        "  max_iter = max_iter, # The number of iterations to run\n",
        "  init = initial_centroids, # Pass in our initial centroids\n",
        "  random_state = random_state_value # The random state for reproducibility\n",
        ")\n",
        "\n",
        "# 4. Fit the model to our data\n",
        "model = model.fit(X)\n",
        "\n",
        "# 5. Gather predictions\n",
        "el_df['g_hat'] = model.predict(X) # Predicted group\n",
        "\n",
        "# 6. Re-normalize the centers so that\n",
        "# they're on the same scale as our original data\n",
        "centers = model.cluster_centers_ # The computed centers\n",
        "centroid_price = centers[:, 0] * (max_price - min_price) + min_price\n",
        "centroid_sales = centers[:, 1] * (max_sales - min_sales) + min_sales\n",
        "\n",
        "# 7. Plot the data and their centers\n",
        "sns.scatterplot(\n",
        "    data = el_df,\n",
        "    x = 'price',\n",
        "    y = 'sales',\n",
        "    hue = 'g_hat', # Color points based on predicted group\n",
        "    style = 'g_hat'\n",
        ")\n",
        "plt.title('Centroids and Group Predictions After 1 Iterations')\n",
        "\n",
        "## Ploting the centroids\n",
        "plt.scatter(\n",
        "    centroid_price,\n",
        "    centroid_sales,\n",
        "    color = 'red',\n",
        "    label = 'centroids'\n",
        ")\n",
        "\n",
        "# Add a legend and show\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xMMaWFXI0jfW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Iteration 2\n",
        "\n",
        "Now, we'll go back to our first initial centroid placement and view how the algorithm progresses with different iterations."
      ],
      "metadata": {
        "id": "eBzOSE_Qmr4B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Set a seed for reproducibility\n",
        "random_state_value = 12 # using our original seed again\n",
        "np.random.seed(random_state_value)\n",
        "k = 5 # Fill this in with your guess from above\n",
        "max_iter = 2 # Change the number of iterations to 2\n",
        "\n",
        "# 2. Create some initial centroids that are randomly defined\n",
        "initial_centroids = np.random.rand(k, 2)\n",
        "\n",
        "# 3. Create a model instance for the KMeans\n",
        "model = KMeans(\n",
        "  n_clusters = k, # The number of centroids / groups\n",
        "  max_iter = max_iter, # The number of iterations to run\n",
        "  init = initial_centroids, # Pass in our initial centroids\n",
        "  random_state = random_state_value # The random state for reproducibility\n",
        ")\n",
        "\n",
        "# 4. Fit the model to our data\n",
        "model = model.fit(X)\n",
        "\n",
        "# 5. Gather predictions\n",
        "el_df['g_hat'] = model.predict(X) # Predicted group\n",
        "\n",
        "# 6. Re-normalize the centers so that\n",
        "# they're on the same scale as our original data\n",
        "centers = model.cluster_centers_ # The computed centers\n",
        "centroid_price = centers[:, 0] * (max_price - min_price) + min_price\n",
        "centroid_sales = centers[:, 1] * (max_sales - min_sales) + min_sales\n",
        "\n",
        "# 7. Plot the data and their centers\n",
        "sns.scatterplot(\n",
        "    data = el_df,\n",
        "    x = 'price',\n",
        "    y = 'sales',\n",
        "    hue = 'g_hat', # Color points based on predicted group\n",
        "    style = 'g_hat'\n",
        ")\n",
        "plt.title(f'Centroids and Group Predictions After {max_iter} Iterations')\n",
        "\n",
        "## Ploting the centroids\n",
        "plt.scatter(\n",
        "    centroid_price,\n",
        "    centroid_sales,\n",
        "    color = 'red',\n",
        "    label = 'centroids'\n",
        ")\n",
        "\n",
        "# Add a legend and show\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VrMk-RrajSYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Iteration 3"
      ],
      "metadata": {
        "id": "4QxS8g4UmtIH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Set a seed for reproducibility\n",
        "np.random.seed(random_state_value)\n",
        "k = 5 # Fill this in with your guess from above\n",
        "max_iter = 3 # Change the iterations to 3\n",
        "\n",
        "# 2. Create some initial centroids that are randomly defined\n",
        "initial_centroids = np.random.rand(k, 2)\n",
        "\n",
        "# 3. Create a model instance for the KMeans\n",
        "model = KMeans(\n",
        "  n_clusters = k, # The number of centroids / groups\n",
        "  max_iter = max_iter, # The number of iterations to run\n",
        "  init = initial_centroids, # Pass in our initial centroids\n",
        "  random_state = random_state_value # The random state for reproducibility\n",
        ")\n",
        "\n",
        "# 4. Fit the model to our data\n",
        "model = model.fit(X)\n",
        "\n",
        "# 5. Gather predictions\n",
        "el_df['g_hat'] = model.predict(X) # Predicted group\n",
        "\n",
        "# 6. Re-normalize the centers so that\n",
        "# they're on the same scale as our original data\n",
        "centers = model.cluster_centers_ # The computed centers\n",
        "centroid_price = centers[:, 0] * (max_price - min_price) + min_price\n",
        "centroid_sales = centers[:, 1] * (max_sales - min_sales) + min_sales\n",
        "\n",
        "# 7. Plot the data and their centers\n",
        "sns.scatterplot(\n",
        "    data = el_df,\n",
        "    x = 'price',\n",
        "    y = 'sales',\n",
        "    hue = 'g_hat', # Color points based on predicted group\n",
        "    style = 'g_hat'\n",
        ")\n",
        "plt.title(f'Centroids and Group Predictions After {max_iter} Iterations')\n",
        "\n",
        "## Ploting the centroids\n",
        "plt.scatter(\n",
        "    centroid_price,\n",
        "    centroid_sales,\n",
        "    color = 'red',\n",
        "    label = 'centroids'\n",
        ")\n",
        "\n",
        "# Add a legend and show\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wS0mySRLjaa9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Iteration 4"
      ],
      "metadata": {
        "id": "7WfrbwDrmuS1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Set a seed for reproducibility\n",
        "np.random.seed(random_state_value)\n",
        "k = 5 # Fill this in with your guess from above\n",
        "max_iter = 4 # Change the iterations to 4\n",
        "\n",
        "# 2. Create some initial centroids that are randomly defined\n",
        "initial_centroids = np.random.rand(k, 2)\n",
        "\n",
        "# 3. Create a model instance for the KMeans\n",
        "model = KMeans(\n",
        "  n_clusters = k, # The number of centroids / groups\n",
        "  max_iter = max_iter, # The number of iterations to run\n",
        "  init = initial_centroids, # Pass in our initial centroids\n",
        "  random_state = random_state_value # The random state for reproducibility\n",
        ")\n",
        "\n",
        "# 4. Fit the model to our data\n",
        "model = model.fit(X)\n",
        "\n",
        "# 5. Gather predictions\n",
        "el_df['g_hat'] = model.predict(X) # Predicted group\n",
        "\n",
        "# 6. Re-normalize the centers so that\n",
        "# they're on the same scale as our original data\n",
        "centers = model.cluster_centers_ # The computed centers\n",
        "centroid_price = centers[:, 0] * (max_price - min_price) + min_price\n",
        "centroid_sales = centers[:, 1] * (max_sales - min_sales) + min_sales\n",
        "\n",
        "# 7. Plot the data and their centers\n",
        "sns.scatterplot(\n",
        "    data = el_df,\n",
        "    x = 'price',\n",
        "    y = 'sales',\n",
        "    hue = 'g_hat', # Color points based on predicted group\n",
        "    style = 'g_hat'\n",
        ")\n",
        "plt.title(f'Centroids and Group Predictions After {max_iter} Iterations')\n",
        "\n",
        "## Ploting the centroids\n",
        "plt.scatter(\n",
        "    centroid_price,\n",
        "    centroid_sales,\n",
        "    color = 'red',\n",
        "    label = 'centroids'\n",
        ")\n",
        "\n",
        "# Add a legend and show\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-d98a336jgG5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Iteration 5"
      ],
      "metadata": {
        "id": "C5V5yhsEmvnd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Set a seed for reproducibility\n",
        "np.random.seed(random_state_value)\n",
        "k = 5 # Fill this in with your guess from above\n",
        "max_iter = 5 # Change the iterations to 5\n",
        "\n",
        "# 2. Create some initial centroids that are randomly defined\n",
        "initial_centroids = np.random.rand(k, 2)\n",
        "\n",
        "# 3. Create a model instance for the KMeans\n",
        "model = KMeans(\n",
        "  n_clusters = k, # The number of centroids / groups\n",
        "  max_iter = max_iter, # The number of iterations to run\n",
        "  init = initial_centroids, # Pass in our initial centroids\n",
        "  random_state = random_state_value # The random state for reproducibility\n",
        ")\n",
        "\n",
        "# 4. Fit the model to our data\n",
        "model = model.fit(X)\n",
        "\n",
        "# 5. Gather predictions\n",
        "el_df['g_hat'] = model.predict(X) # Predicted group\n",
        "\n",
        "# 6. Re-normalize the centers so that\n",
        "# they're on the same scale as our original data\n",
        "centers = model.cluster_centers_ # The computed centers\n",
        "centroid_price = centers[:, 0] * (max_price - min_price) + min_price\n",
        "centroid_sales = centers[:, 1] * (max_sales - min_sales) + min_sales\n",
        "\n",
        "# 7. Plot the data and their centers\n",
        "sns.scatterplot(\n",
        "    data = el_df,\n",
        "    x = 'price',\n",
        "    y = 'sales',\n",
        "    hue = 'g_hat', # Color points based on predicted group\n",
        "    style = 'g_hat'\n",
        ")\n",
        "plt.title(f'Centroids and Group Predictions After {max_iter} Iterations')\n",
        "\n",
        "## Ploting the centroids\n",
        "plt.scatter(\n",
        "    centroid_price,\n",
        "    centroid_sales,\n",
        "    color = 'red',\n",
        "    label = 'centroids'\n",
        ")\n",
        "\n",
        "# Add a legend and show\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kDmwxq7Mjykg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Example with 100 Iterations"
      ],
      "metadata": {
        "id": "CvbR4Il-mx2j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Set a seed for reproducibility\n",
        "np.random.seed(random_state_value)\n",
        "k = 5 # Fill this in with your guess from above\n",
        "max_iter = 100 # Change the iterations to 100\n",
        "\n",
        "# 2. Create some initial centroids that are randomly defined\n",
        "initial_centroids = np.random.rand(k, 2)\n",
        "\n",
        "# 3. Create a model instance for the KMeans\n",
        "model = KMeans(\n",
        "  n_clusters = k, # The number of centroids / groups\n",
        "  max_iter = max_iter, # The number of iterations to run\n",
        "  init = initial_centroids, # Pass in our initial centroids\n",
        "  random_state = random_state_value # The random state for reproducibility\n",
        ")\n",
        "\n",
        "# 4. Fit the model to our data\n",
        "model = model.fit(X)\n",
        "\n",
        "# 5. Gather predictions\n",
        "el_df['g_hat'] = model.predict(X) # Predicted group\n",
        "\n",
        "# 6. Re-normalize the centers so that\n",
        "# they're on the same scale as our original data\n",
        "centers = model.cluster_centers_ # The computed centers\n",
        "centroid_price = centers[:, 0] * (max_price - min_price) + min_price\n",
        "centroid_sales = centers[:, 1] * (max_sales - min_sales) + min_sales\n",
        "\n",
        "# 7. Plot the data and their centers\n",
        "sns.scatterplot(\n",
        "    data = el_df,\n",
        "    x = 'price',\n",
        "    y = 'sales',\n",
        "    hue = 'g_hat', # Color points based on predicted group\n",
        "    style = 'g_hat'\n",
        ")\n",
        "plt.title(f'Centroids and Group Predictions After {max_iter} Iterations')\n",
        "\n",
        "## Ploting the centroids\n",
        "plt.scatter(\n",
        "    centroid_price,\n",
        "    centroid_sales,\n",
        "    color = 'red',\n",
        "    label = 'centroids'\n",
        ")\n",
        "\n",
        "# Add a legend and show\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vbaDLXHEmoxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Greedy Algorithms\n",
        "\n",
        "- KMeans Clustering is an example of a *greedy algorithm*.\n",
        "- Each time we assign observations to a cluster or centroid, we ignore the consequences of changing the centroid averages for overal optimality.\n",
        "- Because we ignore the consequences of changing the centroid averages, the **optimal assignment might change**.\n",
        "- **KMeans May Never Converge:** Since this is a greedy algorithm, KMeans may not converge and can run endlessly. In this case, the points will continually jump back and forth between centroid assignments. There is no guarentee for a stable solution.\n",
        "- **Short-sighted:** The algorithm is short-sighted about the consequences of its action, and need not converge to an optimal outcome.\n",
        "\n",
        "## Changing the value of $k$\n",
        "\n",
        "- We want to find an optimal value of k such that we do not have too many clusters.\n",
        "- Too many clusters means that our initial guesses give similar results in terms of minimizing error, but the clusters change a lot across initial guess. This means that our assignments are somewhat arbitrary, defeating the point of the algorithm.\n",
        "\n",
        "- **Let's look at the results we get for different values of k**"
      ],
      "metadata": {
        "id": "AIE-BXYynqnt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Examle with k = 2"
      ],
      "metadata": {
        "id": "TNFAqohRq4YG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's try\n",
        "# 1. Set a seed for reproducibility\n",
        "np.random.seed(random_state_value)\n",
        "k = 2 # Example with 2 centroids\n",
        "max_iter = 300 # Change the iterations to 300\n",
        "\n",
        "# 2. Create some initial centroids that are randomly defined\n",
        "initial_centroids = np.random.rand(k, 2)\n",
        "\n",
        "# 3. Create a model instance for the KMeans\n",
        "model = KMeans(\n",
        "  n_clusters = k, # The number of centroids / groups\n",
        "  max_iter = max_iter, # The number of iterations to run\n",
        "  init = initial_centroids, # Pass in our initial centroids\n",
        "  random_state = random_state_value # The random state for reproducibility\n",
        ")\n",
        "\n",
        "# 4. Fit the model to our data\n",
        "model = model.fit(X)\n",
        "\n",
        "# 5. Gather predictions\n",
        "el_df['g_hat'] = model.predict(X) # Predicted group\n",
        "\n",
        "# 6. Re-normalize the centers so that\n",
        "# they're on the same scale as our original data\n",
        "centers = model.cluster_centers_ # The computed centers\n",
        "centroid_price = centers[:, 0] * (max_price - min_price) + min_price\n",
        "centroid_sales = centers[:, 1] * (max_sales - min_sales) + min_sales\n",
        "\n",
        "# 7. Plot the data and their centers\n",
        "sns.scatterplot(\n",
        "    data = el_df,\n",
        "    x = 'price',\n",
        "    y = 'sales',\n",
        "    hue = 'g_hat', # Color points based on predicted group\n",
        "    style = 'g_hat'\n",
        ")\n",
        "plt.title(f'Centroid Placements After {max_iter} Iterations\\nk = {k}')\n",
        "\n",
        "## Ploting the centroids\n",
        "plt.scatter(\n",
        "    centroid_price,\n",
        "    centroid_sales,\n",
        "    color = 'red',\n",
        "    label = 'centroids'\n",
        ")\n",
        "\n",
        "# Add a legend and show\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "e8x1gVL2qQnn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Example with k = 3"
      ],
      "metadata": {
        "id": "cpKdY475q8i4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's try\n",
        "# 1. Set a seed for reproducibility\n",
        "np.random.seed(random_state_value)\n",
        "k = 3 # Example with 3 centroids\n",
        "max_iter = 300 # Change the iterations to 300\n",
        "\n",
        "# 2. Create some initial centroids that are randomly defined\n",
        "initial_centroids = np.random.rand(k, 2)\n",
        "\n",
        "# 3. Create a model instance for the KMeans\n",
        "model = KMeans(\n",
        "  n_clusters = k, # The number of centroids / groups\n",
        "  max_iter = max_iter, # The number of iterations to run\n",
        "  init = initial_centroids, # Pass in our initial centroids\n",
        "  random_state = random_state_value # The random state for reproducibility\n",
        ")\n",
        "\n",
        "# 4. Fit the model to our data\n",
        "model = model.fit(X)\n",
        "\n",
        "# 5. Gather predictions\n",
        "el_df['g_hat'] = model.predict(X) # Predicted group\n",
        "\n",
        "# 6. Re-normalize the centers so that\n",
        "# they're on the same scale as our original data\n",
        "centers = model.cluster_centers_ # The computed centers\n",
        "centroid_price = centers[:, 0] * (max_price - min_price) + min_price\n",
        "centroid_sales = centers[:, 1] * (max_sales - min_sales) + min_sales\n",
        "\n",
        "# 7. Plot the data and their centers\n",
        "sns.scatterplot(\n",
        "    data = el_df,\n",
        "    x = 'price',\n",
        "    y = 'sales',\n",
        "    hue = 'g_hat', # Color points based on predicted group\n",
        "    style = 'g_hat'\n",
        ")\n",
        "plt.title(f'Centroid Placements After {max_iter} Iterations\\nk = {k}')\n",
        "\n",
        "## Ploting the centroids\n",
        "plt.scatter(\n",
        "    centroid_price,\n",
        "    centroid_sales,\n",
        "    color = 'red',\n",
        "    label = 'centroids'\n",
        ")\n",
        "\n",
        "# Add a legend and show\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kAUvY8Daq7ui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Example with k = 4"
      ],
      "metadata": {
        "id": "bIjubfTvrVY6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's try\n",
        "# 1. Set a seed for reproducibility\n",
        "np.random.seed(random_state_value)\n",
        "k = 4 # Example with 4 centroids\n",
        "max_iter = 300 # Change the iterations to 300\n",
        "\n",
        "# 2. Create some initial centroids that are randomly defined\n",
        "initial_centroids = np.random.rand(k, 2)\n",
        "\n",
        "# 3. Create a model instance for the KMeans\n",
        "model = KMeans(\n",
        "  n_clusters = k, # The number of centroids / groups\n",
        "  max_iter = max_iter, # The number of iterations to run\n",
        "  init = initial_centroids, # Pass in our initial centroids\n",
        "  random_state = random_state_value # The random state for reproducibility\n",
        ")\n",
        "\n",
        "# 4. Fit the model to our data\n",
        "model = model.fit(X)\n",
        "\n",
        "# 5. Gather predictions\n",
        "el_df['g_hat'] = model.predict(X) # Predicted group\n",
        "\n",
        "# 6. Re-normalize the centers so that\n",
        "# they're on the same scale as our original data\n",
        "centers = model.cluster_centers_ # The computed centers\n",
        "centroid_price = centers[:, 0] * (max_price - min_price) + min_price\n",
        "centroid_sales = centers[:, 1] * (max_sales - min_sales) + min_sales\n",
        "\n",
        "# 7. Plot the data and their centers\n",
        "sns.scatterplot(\n",
        "    data = el_df,\n",
        "    x = 'price',\n",
        "    y = 'sales',\n",
        "    hue = 'g_hat', # Color points based on predicted group\n",
        "    style = 'g_hat'\n",
        ")\n",
        "plt.title(f'Centroid Placements After {max_iter} Iterations\\nk = {k}')\n",
        "\n",
        "## Ploting the centroids\n",
        "plt.scatter(\n",
        "    centroid_price,\n",
        "    centroid_sales,\n",
        "    color = 'red',\n",
        "    label = 'centroids'\n",
        ")\n",
        "\n",
        "# Add a legend and show\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cdjnVwz-rDCu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Example with k = 5"
      ],
      "metadata": {
        "id": "UTW1h9TurWuU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's try\n",
        "# 1. Set a seed for reproducibility\n",
        "np.random.seed(random_state_value)\n",
        "k = 5 # Example with 5 centroids\n",
        "max_iter = 300 # Change the iterations to 300\n",
        "\n",
        "# 2. Create some initial centroids that are randomly defined\n",
        "initial_centroids = np.random.rand(k, 2)\n",
        "\n",
        "# 3. Create a model instance for the KMeans\n",
        "model = KMeans(\n",
        "  n_clusters = k, # The number of centroids / groups\n",
        "  max_iter = max_iter, # The number of iterations to run\n",
        "  init = initial_centroids, # Pass in our initial centroids\n",
        "  random_state = random_state_value # The random state for reproducibility\n",
        ")\n",
        "\n",
        "# 4. Fit the model to our data\n",
        "model = model.fit(X)\n",
        "\n",
        "# 5. Gather predictions\n",
        "el_df['g_hat'] = model.predict(X) # Predicted group\n",
        "\n",
        "# 6. Re-normalize the centers so that\n",
        "# they're on the same scale as our original data\n",
        "centers = model.cluster_centers_ # The computed centers\n",
        "centroid_price = centers[:, 0] * (max_price - min_price) + min_price\n",
        "centroid_sales = centers[:, 1] * (max_sales - min_sales) + min_sales\n",
        "\n",
        "# 7. Plot the data and their centers\n",
        "sns.scatterplot(\n",
        "    data = el_df,\n",
        "    x = 'price',\n",
        "    y = 'sales',\n",
        "    hue = 'g_hat', # Color points based on predicted group\n",
        "    style = 'g_hat'\n",
        ")\n",
        "plt.title(f'Centroid Placements After {max_iter} Iterations\\nk = {k}')\n",
        "\n",
        "## Ploting the centroids\n",
        "plt.scatter(\n",
        "    centroid_price,\n",
        "    centroid_sales,\n",
        "    color = 'red',\n",
        "    label = 'centroids'\n",
        ")\n",
        "\n",
        "# Add a legend and show\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZGGpC2barINB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Example with k = 6"
      ],
      "metadata": {
        "id": "PEnEeULdrYjg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's try\n",
        "# 1. Set a seed for reproducibility\n",
        "np.random.seed(random_state_value)\n",
        "k = 6 # Example with 6 centroids\n",
        "max_iter = 300 # Change the iterations to 300\n",
        "\n",
        "# 2. Create some initial centroids that are randomly defined\n",
        "initial_centroids = np.random.rand(k, 2)\n",
        "\n",
        "# 3. Create a model instance for the KMeans\n",
        "model = KMeans(\n",
        "  n_clusters = k, # The number of centroids / groups\n",
        "  max_iter = max_iter, # The number of iterations to run\n",
        "  init = initial_centroids, # Pass in our initial centroids\n",
        "  random_state = random_state_value # The random state for reproducibility\n",
        ")\n",
        "\n",
        "# 4. Fit the model to our data\n",
        "model = model.fit(X)\n",
        "\n",
        "# 5. Gather predictions\n",
        "el_df['g_hat'] = model.predict(X) # Predicted group\n",
        "\n",
        "# 6. Re-normalize the centers so that\n",
        "# they're on the same scale as our original data\n",
        "centers = model.cluster_centers_ # The computed centers\n",
        "centroid_price = centers[:, 0] * (max_price - min_price) + min_price\n",
        "centroid_sales = centers[:, 1] * (max_sales - min_sales) + min_sales\n",
        "\n",
        "# 7. Plot the data and their centers\n",
        "sns.scatterplot(\n",
        "    data = el_df,\n",
        "    x = 'price',\n",
        "    y = 'sales',\n",
        "    hue = 'g_hat', # Color points based on predicted group\n",
        "    style = 'g_hat'\n",
        ")\n",
        "plt.title(f'Centroid Placements After {max_iter} Iterations\\nk = {k}')\n",
        "\n",
        "## Ploting the centroids\n",
        "plt.scatter(\n",
        "    centroid_price,\n",
        "    centroid_sales,\n",
        "    color = 'red',\n",
        "    label = 'centroids'\n",
        ")\n",
        "\n",
        "# Add a legend and show\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Xmm5ZFywrL5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Example with k = 20"
      ],
      "metadata": {
        "id": "WdBCny4hraLq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's try\n",
        "# 1. Set a seed for reproducibility\n",
        "np.random.seed(random_state_value)\n",
        "k = 20 # Example with 4 centroids\n",
        "max_iter = 300 # Change the iterations to 300\n",
        "\n",
        "# 2. Create some initial centroids that are randomly defined\n",
        "initial_centroids = np.random.rand(k, 2)\n",
        "\n",
        "# 3. Create a model instance for the KMeans\n",
        "model = KMeans(\n",
        "  n_clusters = k, # The number of centroids / groups\n",
        "  max_iter = max_iter, # The number of iterations to run\n",
        "  init = initial_centroids, # Pass in our initial centroids\n",
        "  random_state = random_state_value # The random state for reproducibility\n",
        ")\n",
        "\n",
        "# 4. Fit the model to our data\n",
        "model = model.fit(X)\n",
        "\n",
        "# 5. Gather predictions\n",
        "el_df['g_hat'] = model.predict(X) # Predicted group\n",
        "\n",
        "# 6. Re-normalize the centers so that\n",
        "# they're on the same scale as our original data\n",
        "centers = model.cluster_centers_ # The computed centers\n",
        "centroid_price = centers[:, 0] * (max_price - min_price) + min_price\n",
        "centroid_sales = centers[:, 1] * (max_sales - min_sales) + min_sales\n",
        "\n",
        "# 7. Plot the data and their centers\n",
        "sns.scatterplot(\n",
        "    data = el_df,\n",
        "    x = 'price',\n",
        "    y = 'sales',\n",
        "    hue = 'g_hat', # Color points based on predicted group\n",
        "    style = 'g_hat'\n",
        ")\n",
        "plt.title(f'Centroid Placements After {max_iter} Iterations\\nk = {k}')\n",
        "\n",
        "## Ploting the centroids\n",
        "plt.scatter(\n",
        "    centroid_price,\n",
        "    centroid_sales,\n",
        "    color = 'red',\n",
        "    # label = 'centroids'\n",
        ")\n",
        "\n",
        "# Add a legend and show\n",
        "plt.legend(bbox_to_anchor=(1.05, 1.25))\n",
        "#plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B0IjjtfNrPuK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sum of Squared Error\n",
        "\n",
        "- We are interested in understanding how well our identified clusters fit the data. To do this, we can look at the *sum of squared error*.\n",
        "\n",
        "- When calcualting the sum of squared error, we fix the value for k and focus on its solution to the clustering problem.\n",
        "\n",
        "- Each cluster $j$ has a centroid $c_j$. We can define the distance from each observation $x_i$ to its assigned centroid as $d(x_i, c_j)$.\n",
        "\n",
        "- The **within cluster squared error** is calculated as:\n",
        "\n",
        "\\begin{gather}\n",
        "  W_j = \\sum_{\\text{All observations } i \\text{ in cluster } j} d(x_i,c_j)^2\n",
        "\\end{gather}\n",
        "\n",
        "* In words, the **within cluster squared error** is just the sum of squared distances of points in a single cluster to its centroid.\n",
        "\n",
        "* The **sum of squared error (SSE)** is\n",
        "\n",
        "\\begin{gather}\n",
        "  \\sum_{\\text{All clusters j}} W_j = \\sum_{\\text{All clusters j}} \\quad \\sum_{\\text{All observations } i \\text{ in cluster } j} d(x_i,c_j)^2\n",
        "\\end{gather}\n",
        "\n",
        "* In words, the SSE is the sum of the within cluster squared errors.\n",
        "\n",
        "* In principle, KMC is trying to minimize the SSE, but it is a greedy algorithm, so it doesn't achieve the global minimum SSE.\n",
        "\n",
        "* In scikit, the SSE is calculated and stored in the fitted model. It is stored as an attribute called `.inertia_`.\n",
        "\n"
      ],
      "metadata": {
        "id": "LSjJ698bsMNO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Identifying the best value of k using the Scree Plot\n",
        "\n",
        "* We can plot the SSE for each value of k, $SSE(k)$, against the number of clusters k.\n",
        "* This creates a **scree plot** which allows us to decide what the best value of k is for our data."
      ],
      "metadata": {
        "id": "yK3Z46IPtp9k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select a list of k's that we want to investigate\n",
        "max_k = 10\n",
        "k_grid = np.arange(1, max_k + 1)\n",
        "\n",
        "# Create a Numpy array to store our values of SSE\n",
        "SSE = np.zeros(max_k)\n",
        "\n",
        "# Loop over the indices for each value of k\n",
        "for j in range(max_k):\n",
        "\n",
        "  # index out the value of k\n",
        "  k = k_grid[j]\n",
        "\n",
        "  # Create the model instance using our value of k\n",
        "  model = KMeans(\n",
        "      n_clusters = k,\n",
        "      max_iter = 300, # Define the number of iterations to run\n",
        "      n_init = 10 # Define the number of initializations\n",
        "      # sklearn will return the best model in terms of SSE\n",
        "  )\n",
        "\n",
        "  # Fit the model to our data\n",
        "  model = model.fit(X)\n",
        "\n",
        "  # Isolate the intertia (SSE) for this value of k and save it to our numpy array\n",
        "  SSE[j] = model.inertia_\n",
        "\n",
        "# Create the scree plot using Seaborn\n",
        "sns.lineplot(\n",
        "  x = k_grid,\n",
        "  y = SSE\n",
        ")\n",
        "plt.title('Scree Plot')\n",
        "plt.xlabel('k')\n",
        "plt.ylabel('SSE')\n",
        "plt.xticks(np.arange(1, max_k + 1))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RJDTu8ewtSMI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Deciding the Best Value of k From the Scree Plot\n",
        "\n",
        "- The optimal $k^*$ is decided by looking for large reductions in the SSE by going from $k-1$ to $k$ as compared to going from $k$ to $k+1$.\n",
        "\n",
        "- In other words, we want the value of k that we choose to significantly decrease the value of the SSE and for adding additional clusters after that value of k to only marginally decrease the SSE.\n",
        "\n",
        "- This value $k^*$ is considered the **elbow** in the Scree plot because there's a large drop in the SSE followed by a marginal decrease in SSE as k increases.\n",
        "\n",
        "- **What does it mean if there's no elbow in the scree plot?**\n",
        "  1. There may not be discrete clusters in your data. Instead, there's a continuous trend.\n",
        "  2. It's possible there are discrete clusters, but you need to do more feature engineering (e.g. Principle Component Analysis), or use a different algoritm or error metric to identify them.\n",
        "\n",
        "- The scree plot is pretty subjective, and more quantitative approches exist such as the [silhouette score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html).\n",
        "\n",
        "**Question:** From the plot above, what do you think the best value of $k$ is for our electricity data?"
      ],
      "metadata": {
        "id": "laSyWT2Zv_WJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Identifying the Truth in Our Data Example\n",
        "\n",
        "* In this example, we assumed we didn't know the correct answer beforehand for demonstration purposes.\n",
        "* The attribute of our data we were actually estimating with our identified groups was the variable `sectorName`. This is what sector the observation fell into.\n",
        "* Let's plot our data to see what the true split should have been:"
      ],
      "metadata": {
        "id": "HkgOiGWnfqeG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the true values\n",
        "sns.scatterplot(\n",
        "    data = el_df,\n",
        "    x = 'price',\n",
        "    y = 'sales',\n",
        "    hue = 'sectorName' # Color points based on True sectors\n",
        ")\n",
        "plt.title('True Clustering of Our Electrcity Example')\n",
        "plt.xlabel('Price (cents / kWh)')\n",
        "plt.ylabel('Sales (million kWh)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TKYNXfgLyyFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question:** How was your guess for the value of k?"
      ],
      "metadata": {
        "id": "o75IZtJW9a-s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusions\n",
        "\n",
        "* Clustering in high dimensions is risky. There could be redundancy in the information between different variables and the values get further apart in higher dimensions.\n",
        "  - When dealing with higher dimensions, you can use dimensionality reduction techniques to represet your higher dimensional data within a lower dimension. The most commont tehcnique is **Principle Component Analysis**. It also removes redundancy between variables.\n",
        "* There are other versions of clustering that fix some short comings of KMeans clustering (spectral, dbscan, OPTICS, Mahalanobis). These techniques are more complex and computationally expensive.\n",
        "* Clustering can be used to identify a latent structure in your data and then you can use that structure for other analyses."
      ],
      "metadata": {
        "id": "9dXmStmlzWYd"
      }
    }
  ]
}