{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# k-Nearest Neighbors Classification\n",
        "## DS-3001: Machine Learning 1\n",
        "\n",
        "Content adapted from Terence Johnson (UVA)\n",
        "\n",
        "**Notebook Summary**: In this notebook we start to look at a basic machine learning model: k Nearest Neighbors. We talk about supervised vs unsupervised learning, classification vs regression, the KNN Classification Model, data prepration, hyperparameter tuning, overfitting vs underfitting, and model evaluation."
      ],
      "metadata": {
        "id": "UBtlbsj4crCs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Setting Up Google Colab Environment"
      ],
      "metadata": {
        "id": "k68sLP4cc_po"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8YGmfEnbccuc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os # For changing directory\n",
        "\n",
        "# Load in from Sklearn\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# To mount your google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# path_to_DS_3001_folder = '/content/drive/MyDrive/DS-3001/02_Intro_to_ML_Algorithms'\n",
        "path_to_DS_3001_folder = ''\n",
        "\n",
        "# Update the path to your folder for the class\n",
        "# Where you stored the data from the previous noteboook\n",
        "# path_to_DS_3001_folder = ''\n",
        "os.chdir(path_to_DS_3001_folder)"
      ],
      "metadata": {
        "id": "QUK42-s4dG9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Supervised Learning\n",
        "\n",
        "* *Supervised learning* is a type of machine learning where **we have the true values for what the outcome should be for each observation in our training data set**. Ex. If training a model to predict whether a patient has diabetes or not, our training data has knowledge of whether the patient has diabetes or not.\n",
        "\n",
        "* *Unsupervised learning* is type of machine learning where **we do not have access to the true labels of our data even in our training data set**. Ex. We have data for cells and we want to identify different cell types, but we don't know what those cell types are beforehand. This is something we'll discuss in the coming weeks.\n",
        "\n",
        "* The general set up for supervised learning is:\n",
        "  - There is a target/outcome variable $y$, which we are trying to predict using features/covaraites $X$.\n",
        "  - Our machine learning models have the following premise: \"If $X$ and $X'$ are similar, then $y$ and $y'$ are probably similar.\"\n",
        "  - A **predictive model** is the mapping from features/covariates $\\hat{x}$ to target/outcome $\\hat{y}$, so $\\hat{y} = m(\\hat{x})$ where $m()$ is our model."
      ],
      "metadata": {
        "id": "8CSv_SQEdciV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classification vs Regression\n",
        "\n",
        "* Whether the task is **classification** or **regression** depends on the data type of your output prediction.\n",
        "\n",
        "* **Classification:** When we are predicting a categorical variable. Ex. predicting whether a patient has diabetes or not.\n",
        "\n",
        "* **Regression:** When we are predicting a numeric variable. Usually conitnuous. Ex. Predicting the stock price of a company at a time point.\n",
        "\n",
        "* We're going to use a simplistic model to explore both classification and regression. The model we're exploring is called **k Nearest Neighbors (k-NN or KNN)**.\n"
      ],
      "metadata": {
        "id": "alBBsPsdiWUJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## $k$-NN Classification\n",
        "\n",
        "- Imagine we have a categorical variable $y$ of interest, that we want to predict based on features/covariates $X$: Disease based on patient symptoms, corporate or sovereign rating based on financial indicators, genus or species based on characteristics or DNA\n",
        "- This is a classical **classification** problem: Given variables $X$, what is the probability we would assign to each possible label $\\ell$ for the variable $y$?\n",
        "\n",
        "- Imagine we have covariates/features $X = [x_1, x_2, ..., x_L]$, consisting of $N$ observations of $L$ variables, and observed outcomes/target variable $Y$\n",
        "- Consider a new case $\\hat{x} = (\\hat{x}_1,...,\\hat{x}_L)$. We want to make a guess of what **class/label** it will likely take, $\\hat{\\ell}$\n",
        "- The *$k$ Nearest Neighbor Classification Algorithm* is:\n",
        "  1. Compute the distance from $\\hat{x}$ to each observation $x_i$ in the dataset\n",
        "  2. Find the $k$ \"nearest neighbors\" $x_1^*$, $x_2^*$, ..., $x_k^*$ to $\\hat{x}$ in the data in terms of distance, with outcome labels $\\ell_{1}^*$, $\\ell_2^*$, ..., $\\ell_k^*$\n",
        "  3. Return either a\n",
        "        - **Hard Classification**: The modal/most common label among the nearest neighbor labels\n",
        "        - **Soft Classification**: The proportions for which each of the labels occur among the nearest neighbors"
      ],
      "metadata": {
        "id": "2IX-4Ap-m-Iv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How do we define distance between our different observations?\n",
        "\n",
        "- In one dimension, the distance between two values is the absolute value: $d(a,b) = |a-b| = \\sqrt{(a-b)^2}$.\n",
        "\n",
        "- In many dimensions, this is called the **Euclidean distance**. Imagine we have lists of $L$ variables for two observations, $a = [a_1, ..., a_L]$ and $b = [b_1, ..., b_L]$. We can then calculate the distance between a b as:\n",
        "\n",
        "\\begin{gather}\n",
        "  d(a,b) = ||a - b|| \\\\ = \\sqrt{(a_1 - b_1)^2 + (a_2-b_2)^2 + ... + (a_L - b_L)^2}\n",
        "\\end{gather}"
      ],
      "metadata": {
        "id": "vUQHTA0fhdxf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Examples in code\n",
        "\n",
        "# 1. 1-Dimension Example\n",
        "# Distance between 8 and 12\n",
        "print('Distance between 8 and 12:', np.abs(8-12))\n",
        "\n",
        "# 2. 2-Dimension Example\n",
        "# Distance bewteen (1,3) and (4,-2)\n",
        "print('Distance between (1,3) and (4,-2):', np.sqrt((1-4)**2 + (3 - -2)**2))\n",
        "\n",
        "# 3. Easier Way to Calculate Distances in 2D using Numpy\n",
        "a = np.array([1, 3])\n",
        "b = np.array([4, -2])\n",
        "\n",
        "print('Distance between (1,3) and (4,-2), Numpy:', np.linalg.norm(a - b))\n",
        "\n",
        "# 4. In 5-dimensions\n",
        "a = np.array([10, 2, 0, 3, 24])\n",
        "b = np.array([8, 10, 4, 9, 15])\n",
        "\n",
        "print('Distance between 5-Dimensional Arrays:', np.linalg.norm(a-b))"
      ],
      "metadata": {
        "id": "XILa1dC0h19A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How do we consider the difference in scale for each of our variables?\n",
        "\n",
        "- Often times, our variables will have different units and scales. For example, the variables we use may be age, height, weight which have units of years, feet, and lbs. These different units have different scales, which thus would affect our calcultions of distance unequally.\n",
        "\n",
        "- Because of this, we need to normalize or scale each of our features so that they're on a similar scale and don't unequally affect the distance calculation.\n",
        "\n",
        "- One way to do this is the **Min-Max scale**. For a variable $x$ we can compute the scaled version of the variable ($u$). For each observation i, we calculate:\n",
        "\n",
        "\\begin{gather}\n",
        "  u_i = \\frac{x_i - \\text{min}(x)}{\\text{max}(x) - \\text{min}(x)}\n",
        "\\end{gather}\n",
        "\n",
        "- This transforms all the values of $x$ so that that they lie between 0 and 1, with the smallest value mapped to 0 and the largest value mapped to 1.\n",
        "\n",
        "- If you do not do this (or if the variables aren't already similarly scaled), neighbor methods simple won't work correctly. One variable may be unequally weighted over another.\n",
        "\n",
        "- It's easier to transform the data and keep the original, untransformed version to compare with predictions later.\n"
      ],
      "metadata": {
        "id": "N5kkHXXdo5Hf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's create a MinMaxScaler function\n",
        "def MinMaxScaler(x):\n",
        "\n",
        "  # Pre-compute the min and max of the variable\n",
        "  min_x = np.min(x)\n",
        "  max_x = np.max(x)\n",
        "\n",
        "  # Calculate the newly scaled version of the variable\n",
        "  u = (x - min_x) / (max_x - min_x)\n",
        "\n",
        "  # Return the scaled version of the value\n",
        "  return u"
      ],
      "metadata": {
        "id": "ktq0i7s6o96m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SciKit-Learn\n",
        "- Unless we are doing something tailored to a particular task, we generally don't want to code our own algorithms: It is time consuming, and existing implementations are typically more efficient or robust than what we would create (but there is a lot of value in coding your own algorithms and estimators)\n",
        "- The most popular Python machine learning library is called **SciKit-Learn**\n",
        "- You typically import it as\n",
        "  - `from sklearn.<model class> import <model name>`\n",
        "  - where `<model class>` is the set of related models and `<model name>` is the desired algorithm\n",
        "\n",
        "- For k nearest neighbors we imported it as such, where neighbors is our `<model class>` and KNeighborsClassifier is our `<model name>`:\n",
        "  - `from sklearn.neighbors import KNeighborsClassifier`\n",
        "\n",
        "#### General Workflow for Sklearn with KNN Classification\n",
        "\n",
        "1. Create an untrained model object with a fixed $k$:\n",
        "  * For classification: `model = KNeighborsClassifier(n_neighbors=k)`\n",
        "  * For regression: `model = KNeighborsRegressor(n_neighbors=k)`\n",
        "\n",
        "\n",
        "2. Fit that object to the data, $(X,y)$:\n",
        "  * `fitted_model = model.fit(X,y)`\n",
        "\n",
        "3. Use the fitted object to make predictions for new cases $\\hat{x}$ as either hard or soft classifications:\n",
        "  * Hard Classification: `y_hat = fitted_model.predict(x_hat)`\n",
        "  * Soft Classification: `y_hat = fitted_model.predict_proba(x_hat)`"
      ],
      "metadata": {
        "id": "XENy2iHnuIGh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Workflow for fitting a of KNN Classifier Model\n",
        "\n",
        "### 1. Loading in Our Data\n",
        "\n",
        "* You can get the data from the GitHub repository for this class.\n",
        "\n",
        "* For this notebook, we'll work with some data for classifying whether a patient tested positve for diabetes or not. The data come from female patients at least 21 years old of Pima Indian heritage. The available features are:\n",
        "\n",
        "1. **Pregnancies**: Number of times pregnant\n",
        "1. **Glucose**: Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n",
        "1. **BloodPressure**: Diastolic blood pressure (mm Hg)\n",
        "1. **SkinThickness**: Triceps skin fold thickness (mm)\n",
        "1. **Insulin**: 2-Hour serum insulin (mu U/ml)\n",
        "1. **BMI**: Body mass index (weight in kg/(height in m)^2)\n",
        "1. **DiabetesPedigreeFunction**: Diabetes pedigree function. Indicates the function which scores likelihood of diabetes based on family history.\n",
        "1. **Age**: Age (years)\n"
      ],
      "metadata": {
        "id": "rwFZPxfKuEsq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "diabetes_df = pd.read_csv('./data/diabetes.csv', low_memory = False)\n",
        "diabetes_df.head()"
      ],
      "metadata": {
        "id": "WZdRCgMeuBVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the shape of the data\n",
        "diabetes_df.shape"
      ],
      "metadata": {
        "id": "t-ep4xSmwmvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the data types\n",
        "diabetes_df.dtypes"
      ],
      "metadata": {
        "id": "Gj-_cUo6wxJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Look at unique values of some variables\n",
        "var = 'BloodPressure'\n",
        "diabetes_df[var].unique()\n",
        "diabetes_df[var].plot.hist(bins = 20)\n",
        "\n",
        "# Note that Glucose, BloodPressure, SkinThickness, Insulin, and BMI cannot be 0\n",
        "# so these are effectively nan values. We want to impute them somehow."
      ],
      "metadata": {
        "id": "aH4pxqMfw4Do"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Impute each case where the values are 0 and cannot feasibly be\n",
        "variables_to_impute = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']\n",
        "\n",
        "# Loop over the variables to impute\n",
        "for var in variables_to_impute:\n",
        "  diabetes_df[var] = diabetes_df[var].mask(\n",
        "      diabetes_df[var] == 0, # Boolean for whether the variable equals 0 for observation\n",
        "      diabetes_df[var].median() # Replacing with the median for the variable, robust to outliers\n",
        "  )"
      ],
      "metadata": {
        "id": "0mK9PtEJw-8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the result after the imputation\n",
        "var = 'BloodPressure'\n",
        "diabetes_df[var].unique()\n",
        "diabetes_df[var].plot.hist(bins = 20)"
      ],
      "metadata": {
        "id": "Ck4YseLt1Q56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Select Our Outcome and Feature variables, Normalize the features"
      ],
      "metadata": {
        "id": "71TCgUsT1h7q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select the outcome variable\n",
        "y = diabetes_df['Outcome']\n",
        "\n",
        "# Select the feature variables\n",
        "var1 = 'Glucose'\n",
        "var2 = 'Insulin'\n",
        "features_of_interest = [var1, var2]\n",
        "\n",
        "# Isloate our covariates/features\n",
        "x = diabetes_df.loc[:, features_of_interest]\n",
        "\n",
        "# Scale our variables using the min/max scaler\n",
        "u = x.apply(MinMaxScaler)\n"
      ],
      "metadata": {
        "id": "P4HQd0LI1C2c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare the scales of the original and re-scaled values\n",
        "sns.histplot(x[var1], label = f'{var1}')\n",
        "sns.histplot(x[var2], label = f'{var2}')\n",
        "plt.xlabel(f'Values for {var1} and {var2}')\n",
        "plt.title(f'Unscaled Distributions')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "sns.histplot(u[var1], label = f'{var1}')\n",
        "sns.histplot(u[var2], label = f'{var2}')\n",
        "plt.xlabel(f'Scaled Values for {var1} and {var2}')\n",
        "plt.title(f'Scaled Distributions')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "JZRcSqjV2tFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Look at the scatter points of the two variables of interest to guess what the\n",
        "# knn result will be\n",
        "\n",
        "sns.scatterplot(x = u[var1], y = u[var2], hue = y)\n",
        "plt.title(f'Comparison of Outcome: {var1} vs {var2}')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IKSMtIo44E66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Fit a kNN Classifier and Predict"
      ],
      "metadata": {
        "id": "eiRxNl9y34B0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the number of neighbors, use an odd number to break ties\n",
        "k = 5\n",
        "\n",
        "# Create a fitted model instance\n",
        "model = KNeighborsClassifier(n_neighbors = k)\n",
        "\n",
        "# Fit the model using our scaled data and our outcome variable\n",
        "model = model.fit(u, y)\n",
        "\n",
        "# Make predictions using the fit model\n",
        "y_hat = model.predict(u)"
      ],
      "metadata": {
        "id": "MHba-5Wg3Ed0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Visualize our predictions"
      ],
      "metadata": {
        "id": "b2gM9Sp65qtY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize what we predicted\n",
        "# Here, the color is the true label\n",
        "# The marker style (o vs x) is the prediction\n",
        "sns.scatterplot(\n",
        "    x = u[var1],\n",
        "    y = u[var2],\n",
        "    hue = y, # Setting the marker color based on the truth\n",
        "    style = y_hat # Setting the marker style based on our prediction\n",
        ")\n",
        "\n",
        "plt.title('Comparison of True Label and our Prediction')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "uGVn-gs949Rp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize our predictions in the entire space of possible observations\n",
        "\n",
        "nx = 100 # Number of points on grid for x variable\n",
        "ny = 100 # Number of points on grid for y variable\n",
        "total = nx*ny # Total number of points to plot\n",
        "\n",
        "grid_x = np.linspace(0, 1, nx) # Create a grid of x values\n",
        "grid_y = np.linspace(0, 1, ny) # Create a grid of y values\n",
        "\n",
        "xs, ys = np.meshgrid(grid_x, grid_y) # Explode grids to all possible pairs\n",
        "X = xs.reshape(total) # Turns pairs into vectors\n",
        "Y = ys.reshape(total) # Turns pairs into vectors\n",
        "\n",
        "# Create a dataframe of points to plot\n",
        "x_hat = pd.DataFrame(\n",
        "    {\n",
        "        var1: X,\n",
        "        var2: Y\n",
        "    }\n",
        ")\n",
        "\n",
        "# Fit the model to the points\n",
        "y_hat = model.predict(x_hat)\n",
        "\n",
        "# Add new variable to the dataframe\n",
        "x_hat['y_hat'] = y_hat\n",
        "\n",
        "# Plot the values\n",
        "this_plot = sns.scatterplot(\n",
        "    data=x_hat,\n",
        "    x=var1,\n",
        "    y=var2,\n",
        "    hue='y_hat',\n",
        "    linewidth=0\n",
        ")\n",
        "\n",
        "plt.xlabel(var1)\n",
        "plt.ylabel(var2)\n",
        "plt.title('Prediction Grid Map')\n",
        "\n",
        "# Move legend off the plot canvas\n",
        "sns.move_legend(this_plot, \"upper left\", bbox_to_anchor=(1, 1))"
      ],
      "metadata": {
        "id": "CD22rgk1u1N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Look at whether our model worked: Confusion Matrices and Accuracy\n",
        "\n",
        "* Once we create a model, we want to gain an understanding about whether the model worked.\n",
        "\n",
        "* When we are working with a classification task (ex. whether a patient has diabetes or not), a basic metric to look at is the confusion matrix. We cross-tabulate the true labels and the predicted labels, hoping to see that they align on the diagonal."
      ],
      "metadata": {
        "id": "mfvKwnVp5taf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a cross tab between our true values and the predicted values\n",
        "# We can do normalize to see what percentage of observations fell within\n",
        "# each section\n",
        "cross = pd.crosstab(y, y_hat, normalize = True)\n",
        "cross"
      ],
      "metadata": {
        "id": "ScLYsJ9E5DrU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can visualize this as a heatmap, where color is based on the percentage\n",
        "# of observations in that category\n",
        "sns.heatmap(cross.values, annot = True, cmap = 'Blues')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dhEpgGta7YIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Sometimes we want to simplify this confusion matrix into a single, summary number. The simplest and most obvious method to do this is to look at what proportion of the cases we predicted properly. This is called **accuracy.**\n",
        "\n",
        "* We can calculate the accuracy by summing the numbers on the diagonal of the confusion matrix. If they're raw counts, we need to divide by the total number of observations. If they're already proportions, the sum is sufficient."
      ],
      "metadata": {
        "id": "6o_iezZi7w1J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We can combine this into a single value, the Accuracy\n",
        "# We sum the diagonal of the confusion matrix\n",
        "accuracy = np.sum(np.diag(cross))\n",
        "print('Accuracy:', accuracy)"
      ],
      "metadata": {
        "id": "Yi5lcUmt7Lcq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Selecting $k$\n",
        "\n",
        "### Hyperparameters\n",
        "\n",
        "- For k nearest neighbors, k is a value that we pick and is not learned by the model. This is called a **hyper-parameter**. We don't have an obvious way to pick what the value for k should be just by looking at the data or model itself.\n",
        "- We need to do tests to identify what the best value of k should be for our model. It will have a big effect on the model outcomes.\n",
        "- Usually the most complex models will do the best on the data we have (training data), but they will do a poor job when generalizing to unseen data. We need to strike a delicate balance. In general, we want to find the most parsimonious (simple) model that we can that still does a good job on the test data.\n",
        "\n",
        "\n",
        "Let's look at what happens to the accuracy when we modify the value for k:"
      ],
      "metadata": {
        "id": "vqUpR0xsuv2h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a grid of odd numbered ks\n",
        "k_grid = np.array([(2*k + 1)**2 for k in range(0, 11)])\n",
        "\n",
        "# List to save the accuracies\n",
        "accuracies = []\n",
        "\n",
        "# Loop over the values of k\n",
        "for k in k_grid:\n",
        "  model = KNeighborsClassifier(n_neighbors = k) # Define model instance\n",
        "  model = model.fit(u, y) # Fit the model\n",
        "  y_hat = model.predict(u) # Make predictions\n",
        "  correct = (y == y_hat).astype(int) # Find which predictions are correct\n",
        "\n",
        "  # Plot the values\n",
        "  sns.scatterplot(\n",
        "      x = diabetes_df[var1],\n",
        "      y = diabetes_df[var2],\n",
        "      hue = correct,\n",
        "      style = correct\n",
        "  )\n",
        "  plt.title(f'Comparison of {var1} vs {var2}\\nk = {k}')\n",
        "  plt.show()\n",
        "\n",
        "  # Compute the accuracy\n",
        "  acc = model.score(u, y)\n",
        "\n",
        "  # Save the accuracy\n",
        "  accuracies.append(acc)\n",
        "\n",
        "# Plot the accuracy over time\n",
        "sns.lineplot(x = k_grid, y = accuracies)\n",
        "plt.title('Accuracy vs k')\n",
        "plt.xlabel('k')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2OK3JV_d9w7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Take-away from the above plots?\n",
        "  - From this plot it looks like we should pick k = 1 because it gave us the highest accuracy on our training data set.\n",
        "  - What does this mean in the algorithm? The prediction is based on the single nearest observation in our training data.\n",
        "  - As we increase k, we take into account more nearby points and start to generalize.\n",
        "  - Is it actually true that we should use k=1? How will the perform on unseen data?"
      ],
      "metadata": {
        "id": "ouw5biECzoei"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Overfitting vs Underfitting\n",
        "\n",
        "* Overfitting and underfitting are common phenomenon when training a machine learning model that can negatively impact the outcome, making our models worse in practice.\n",
        "\n",
        "* **Overfitting:** A case when our model is sensitive to a handful of data points found in our training data. In other words, the model learns patterns that are specific to the training data that do not generalize to unseen data. When using the the k nearest neighbors algorithm, this occurs with a small value of k. Ex. $k=1$.\n",
        "\n",
        "* **Underfitting:** A case when the model averages over many observations and gives answers close to population proportions. When using the k nearest neighbors algorithm, this occurs with a large value of k. Ex. $k = 400$"
      ],
      "metadata": {
        "id": "a1emG5kP0Rib"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train-Test Splits\n",
        "\n",
        "- In machine learning, we are interested in **prediction**. We want to have the best prediction for the outcome of new cases after training our model. We don't care as much about how our model performs on data we've already seen, we care more about how our model does on unseen data.\n",
        "\n",
        "- Our plot that we created above is more concerned with how the model performs on data it has already seen. **How can we think about how the model will do on data it has not seen?**\n",
        "\n",
        "- This is where **train-test** splits are handy. We split our data into train and test sets.\n",
        "  - **Train Data:** This is the data that we actually train our model on. This is typically 80% of the data we have on hand, but the exact proportion can vary.\n",
        "  - **Test data**: The data that we test our model on. We calculate the performance metrics (confusion matrices and accruacy) on the test data, simulating how the model would perform on unseen data. This is typically the remaining 20% of the data, but again this proportion can vary.\n",
        "\n",
        "- To perform the train-test split, we will use the `train_test_spilt` function from sklearn. The documentation for the function can be found [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html).\n",
        "  "
      ],
      "metadata": {
        "id": "lCZTdr8B1gjY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sklearn has a function for creating the train test split for us\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "7V7Edy0qyLhN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gathering the train test split, the order of how you catch these outputs matters\n",
        "u_train, u_test, y_train, y_test = train_test_split(\n",
        "    u, # our scaled covariates\n",
        "    y, # our accompanying outcome values\n",
        "    test_size = 0.2, # the proportion of the data\n",
        "    random_state = 24 # the seed for the split, makes the results reproducible\n",
        ")"
      ],
      "metadata": {
        "id": "8oiNzpZR2_iE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the model instance and fit using the TRAIN data\n",
        "model = KNeighborsClassifier(n_neighbors = 5)\n",
        "model = model.fit(u_train, y_train)\n",
        "\n",
        "# Get the prediction on the TEST set\n",
        "y_hat = model.predict(u_test)"
      ],
      "metadata": {
        "id": "JnHolFZH3bZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Look at the confusion matrix\n",
        "cross = pd.crosstab(y_test, y_hat, normalize = True)\n",
        "\n",
        "# We can visualize this as a heatmap, where color is based on the percentage\n",
        "# of observations in that category\n",
        "sns.heatmap(cross.values, annot = True, cmap = 'Blues')\n",
        "plt.title('Confusion Matrix for Test Set')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZUHZVM_9396V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the accuracy for the TEST set\n",
        "acc = model.score(u_test, y_test)\n",
        "print('Accraucy on Test Set for k=5:', acc)"
      ],
      "metadata": {
        "id": "P5ZhaGl64IRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Was our accuracy better or worse than on the test set?**"
      ],
      "metadata": {
        "id": "l54ajxWG4iUe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Selecting $k$ using our Test Data"
      ],
      "metadata": {
        "id": "F23J2mcX4uLw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select out some values of k to look at, keeping them odd\n",
        "k_grid = np.array([(2*k + 3) for k in range(0, 100)]) # Start with a value of 3, 1 is not recommended\n",
        "\n",
        "# Lists to save the values for the\n",
        "# train and test accuracies\n",
        "train_accuracies = []\n",
        "test_accuracies = []\n",
        "\n",
        "# Loop over the values of k\n",
        "for k in k_grid:\n",
        "  # Fitting the model on the TRAIN data\n",
        "  model = KNeighborsClassifier(n_neighbors = k) # Define model instance\n",
        "  model = model.fit(u_train, y_train)\n",
        "\n",
        "  # Predict on the TRAIN data (helps us see how it's doing on the train data)\n",
        "  y_hat_train = model.predict(u_train)\n",
        "  train_acc = model.score(u_train, y_train)\n",
        "\n",
        "  # Predict on the TEST data (what we're most interested)\n",
        "  y_hat_test = model.predict(u_test)\n",
        "  test_acc = model.score(u_test, y_test)\n",
        "\n",
        "  # Append the results so that we can compare them latter\n",
        "  train_accuracies.append(train_acc)\n",
        "  test_accuracies.append(test_acc)\n",
        "\n",
        "  print(f'For k={k}, train accuracy: {train_acc}; test accuracy: {test_acc}')\n",
        "\n",
        "# Plot the accuracy over time\n",
        "sns.lineplot(x = k_grid, y = train_accuracies, label = 'Train Accuracy')\n",
        "sns.lineplot(x = k_grid, y = test_accuracies, label = 'Test Accuracy')\n",
        "plt.title('Accuracy vs k for Train and Testing Data')\n",
        "plt.xlabel('k')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-FIOuBKo4WZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Finding the Optimal k from the above plot"
      ],
      "metadata": {
        "id": "MWMvtwMO8FWp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We want to select the k that gives us the largest test accuracy\n",
        "\n",
        "# Turn the test accuracies into a numpy array\n",
        "test_accuracies = np.array(test_accuracies)\n",
        "\n",
        "# Find where it is maximized\n",
        "best_k_idx = np.argmax(test_accuracies)\n",
        "best_k = k_grid[best_k_idx]\n",
        "\n",
        "print(f'The best k for the test data set is {best_k}')\n"
      ],
      "metadata": {
        "id": "6gE7DZIq7xQ8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}